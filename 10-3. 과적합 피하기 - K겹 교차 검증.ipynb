{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691ea415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# \"conda install scikit-learn\"을 통해 다운로드\n",
    "# 문자열 형태의 예측 결과를 숫자 형태로 바꾸어주는 라이브러리\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cde397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "df = pd.read_csv('deeplearning/dataset/sonar.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79932e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "x = dataset[:, :60]\n",
    "y_obj = dataset[:, 60]\n",
    "\n",
    "e = LabelEncoder()\n",
    "e.fit(y_obj)\n",
    "y = e.transform(y_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce18a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=10, random_state=0, shuffle=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k겹 교차 검증\n",
    "# 10개의 파일로 쪼갤 것\n",
    "\n",
    "n_fold = 10\n",
    "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "skf # 학습셋과 테스트셋의 묶음이 들어가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c49b217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  20\n",
      "  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38  39  41\n",
      "  42  43  45  46  47  48  50  51  53  54  55  56  57  58  59  60  61  62\n",
      "  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
      "  81  82  83  84  85  86  87  88  89  90  92  93  94  95  96  97  98  99\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 118 120\n",
      " 121 123 124 125 126 127 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 142 143 144 145 146 147 148 149 150 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 192 193 194 195 198 199\n",
      " 200 201 202 203 205 206 207]\n",
      "[  2  14  19  22  26  40  44  49  52  91 100 117 119 122 128 141 151 191\n",
      " 196 197 204]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   1   2   3   4   5   7   8   9  10  11  12  14  15  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37  38\n",
      "  39  40  41  42  43  44  47  48  49  50  51  52  53  54  55  57  58  59\n",
      "  61  62  63  64  65  66  68  70  71  72  73  74  75  76  77  78  79  80\n",
      "  81  82  83  84  86  87  88  89  90  91  92  93  94  95  96  98  99 100\n",
      " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 117 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149 150 151 152 153 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 168 169 170 171 172 173 175 176 177 178 179\n",
      " 180 182 183 184 186 187 188 189 190 191 192 193 194 195 196 197 198 200\n",
      " 201 202 203 204 205 206 207]\n",
      "[  6  13  36  45  46  56  60  67  69  85  97 108 116 118 135 154 167 174\n",
      " 181 185 199]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57  59\n",
      "  60  61  62  63  64  65  66  67  68  69  71  72  73  74  76  77  78  80\n",
      "  81  82  83  84  85  86  87  88  90  91  92  93  94  95  96  97  98  99\n",
      " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 132 134 135 136 137 139\n",
      " 140 141 142 143 144 145 146 148 150 151 152 153 154 155 156 157 158 159\n",
      " 160 163 164 165 166 167 168 169 170 172 173 174 175 176 177 178 179 180\n",
      " 181 183 184 185 186 187 188 189 190 191 192 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207]\n",
      "[  0  16  17  35  54  58  70  75  79  89 101 131 133 138 147 149 161 162\n",
      " 171 182 193]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  49  50  51  52  54  55  56  58\n",
      "  59  60  62  63  64  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  81  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 115 116 117 118 119 120\n",
      " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 153 154 155 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 169 170 171 172 173 174 175 176\n",
      " 178 179 181 182 183 184 185 186 187 188 190 191 192 193 194 196 197 198\n",
      " 199 200 201 203 204 205 206]\n",
      "[ 15  18  48  53  57  61  65  80  82  88  99 113 114 152 168 177 180 189\n",
      " 195 202 207]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  21  22  23  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  44  45  46  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  72  73  74  75  76  77  78\n",
      "  79  80  82  83  84  85  86  87  88  89  90  91  92  93  94  97  98  99\n",
      " 100 101 102 103 105 106 108 110 111 112 113 114 115 116 117 118 119 120\n",
      " 121 122 123 124 126 127 128 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 171 173 174 175 176 177 178 179\n",
      " 180 181 182 184 185 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      " 200 201 202 203 204 205 207]\n",
      "[ 20  24  25  42  43  47  71  81  95  96 104 107 109 125 129 144 170 172\n",
      " 183 186 206]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   1   2   4   5   6   7   9  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  35  36  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  52  53  54  56  57  58  60  61\n",
      "  62  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
      "  81  82  83  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
      " 118 119 121 122 123 124 125 126 127 128 129 130 131 132 133 135 136 138\n",
      " 139 140 141 142 143 144 146 147 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 161 162 165 166 167 168 169 170 171 172 173 174 175 176 177 179\n",
      " 180 181 182 183 184 185 186 187 188 189 191 193 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207]\n",
      "[  3   8  10  34  37  51  55  59  63  84 120 134 137 145 160 163 164 178\n",
      " 190 192 194]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   1   2   3   5   6   7   8  10  11  12  13  14  15  16  17  18  19\n",
      "  20  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  39  40\n",
      "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  75  76  77  78\n",
      "  79  80  81  82  84  85  86  87  88  89  90  91  92  95  96  97  98  99\n",
      " 100 101 102 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
      " 120 121 122 123 124 125 128 129 131 132 133 134 135 137 138 139 140 141\n",
      " 143 144 145 146 147 148 149 151 152 153 154 155 156 157 158 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 189 190 191 192 193 194 195 196 197 199 200\n",
      " 201 202 203 204 205 206 207]\n",
      "[  4   9  21  30  38  41  74  83  93  94 103 115 126 127 130 136 142 150\n",
      " 159 188 198]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   1   2   3   4   5   6   8   9  10  11  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
      "  58  59  60  61  62  63  65  66  67  69  70  71  72  73  74  75  76  79\n",
      "  80  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  99\n",
      " 100 101 103 104 105 107 108 109 111 113 114 115 116 117 118 119 120 121\n",
      " 122 123 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 144 145 146 147 148 149 150 151 152 153 154 156 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 185 186 188 189 190 191 192 193 194 195 196 197 198 199\n",
      " 200 202 203 204 205 206 207]\n",
      "[  7  12  29  39  64  68  77  78  86  98 102 106 110 112 124 143 155 157\n",
      " 184 187 201]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  24  25  26  27  29  30  33  34  35  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  88  89  91  92  93  94  95  96  97  98\n",
      "  99 100 101 102 103 104 106 107 108 109 110 112 113 114 115 116 117 118\n",
      " 119 120 122 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 160 161 162 163 164 165 167 168 169 170 171 172 174 175 177 178\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 204 206 207]\n",
      "[  1   5  23  28  31  32  66  87  90 105 111 121 123 140 166 173 176 179\n",
      " 203 205]\n",
      "________________________________________________________________________________________________________________________\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  28  29  30  31  32  34  35  36  37  38\n",
      "  39  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57\n",
      "  58  59  60  61  63  64  65  66  67  68  69  70  71  74  75  77  78  79\n",
      "  80  81  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97  98\n",
      "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n",
      " 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 133 134 135\n",
      " 136 137 138 140 141 142 143 144 145 147 149 150 151 152 154 155 157 159\n",
      " 160 161 162 163 164 166 167 168 170 171 172 173 174 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 201 202 203 204 205 206 207]\n",
      "[ 11  27  33  50  62  72  73  76  92 132 139 146 148 153 156 158 165 169\n",
      " 175 200]\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# sonor.csv 파일은 207행의 데이터로 이우러져 있는데,\n",
    "# 그 중 학습셋으로 이용될 데이터의 행과\n",
    "# 테스트셋으로 이용될 행이 구분되어 출력\n",
    "\n",
    "for train, test in skf.split(x, y):\n",
    "    print(train)\n",
    "    print(test)\n",
    "    print('_' * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33f9adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.01 0.0171 0.0623 ... 0.0044 0.004 0.0117]\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " [0.0124 0.0433 0.0604 ... 0.0196 0.0147 0.0062]\n",
      " [0.0126 0.0149 0.0641 ... 0.0094 0.0116 0.0063]\n",
      " ...\n",
      " [0.005 0.0017 0.027 ... 0.0063 0.0017 0.0028]\n",
      " [0.0366 0.0421 0.0504 ... 0.0017 0.0027 0.0027]\n",
      " [0.0323 0.0101 0.0298 ... 0.0032 0.0062 0.0067]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.0317 0.0956 0.1321 ... 0.0143 0.0036 0.0103]\n",
      " [0.009 0.0062 0.0253 ... 0.0053 0.0189 0.0102]\n",
      " [0.0094 0.0166 0.0398 ... 0.0087 0.0077 0.0122]\n",
      " ...\n",
      " [0.0423 0.0321 0.0709 ... 0.0021 0.0043 0.0017]\n",
      " [0.034 0.0625 0.0381 ... 0.0051 0.0033 0.0058]\n",
      " [0.0116 0.0744 0.0367 ... 0.0044 0.0057 0.0035]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " [0.01 0.0171 0.0623 ... 0.0044 0.004 0.0117]\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0352 0.0116 0.0191 ... 0.0015 0.0073 0.0067]\n",
      " [0.0192 0.0607 0.0378 ... 0.0037 0.0112 0.0075]\n",
      " ...\n",
      " [0.0179 0.0136 0.0408 ... 0.0155 0.016 0.0085]\n",
      " [0.0095 0.0308 0.0539 ... 0.003 0.0031 0.0033]\n",
      " [0.0203 0.0121 0.038 ... 0.0036 0.0013 0.0016]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0323 0.0101 0.0298 ... 0.0032 0.0062 0.0067]\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]]\n",
      "[[0.0298 0.0615 0.065 ... 0.0049 0.02 0.0073]\n",
      " [0.027 0.0092 0.0145 ... 0.0132 0.007 0.0088]\n",
      " [0.019 0.0038 0.0642 ... 0.0022 0.0055 0.0122]\n",
      " ...\n",
      " [0.0129 0.0141 0.0309 ... 0.0017 0.0024 0.0029]\n",
      " [0.0272 0.0378 0.0488 ... 0.0051 0.0065 0.0103]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0323 0.0101 0.0298 ... 0.0032 0.0062 0.0067]\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.0473 0.0509 0.0819 ... 0.0082 0.0028 0.0027]\n",
      " [0.0293 0.0644 0.039 ... 0.016 0.0095 0.0011]\n",
      " [0.0201 0.0026 0.0138 ... 0.0024 0.0057 0.0044]\n",
      " ...\n",
      " [0.0096 0.0404 0.0682 ... 0.0036 0.0043 0.0018]\n",
      " [0.0209 0.0191 0.0411 ... 0.0087 0.007 0.0042]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.01 0.0171 0.0623 ... 0.0044 0.004 0.0117]\n",
      " [0.0223 0.0375 0.0484 ... 0.0093 0.0059 0.0022]\n",
      " [0.0039 0.0063 0.0152 ... 0.0003 0.0053 0.0036]\n",
      " ...\n",
      " [0.0156 0.021 0.0282 ... 0.0056 0.0048 0.0024]\n",
      " [0.0056 0.0267 0.0221 ... 0.0024 0.0034 0.0007]\n",
      " [0.0392 0.0108 0.0267 ... 0.0044 0.0022 0.0014]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.0762 0.0666 0.0481 ... 0.0048 0.0107 0.0094]\n",
      " [0.0164 0.0173 0.0347 ... 0.0035 0.0056 0.004]\n",
      " [0.0664 0.0575 0.0842 ... 0.0162 0.0109 0.0079]\n",
      " ...\n",
      " [0.0235 0.022 0.0167 ... 0.0052 0.0027 0.0021]\n",
      " [0.0089 0.0274 0.0248 ... 0.0069 0.006 0.0018]\n",
      " [0.0238 0.0318 0.0422 ... 0.0013 0.0035 0.006]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.0519 0.0548 0.0842 ... 0.0047 0.0048 0.0053]\n",
      " [0.0079 0.0086 0.0055 ... 0.0058 0.0059 0.0032]\n",
      " [0.0189 0.0308 0.0197 ... 0.0092 0.0052 0.0075]\n",
      " ...\n",
      " [0.0269 0.0383 0.0505 ... 0.0027 0.0055 0.0057]\n",
      " [0.0368 0.0279 0.0103 ... 0.0086 0.011 0.0052]\n",
      " [0.0335 0.0258 0.0398 ... 0.0022 0.0005 0.0031]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " [0.01 0.0171 0.0623 ... 0.0044 0.004 0.0117]\n",
      " ...\n",
      " [0.0323 0.0101 0.0298 ... 0.0032 0.0062 0.0067]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0286 0.0453 0.0277 ... 0.0027 0.0051 0.0062]\n",
      " [0.0115 0.015 0.0136 ... 0.0037 0.007 0.0041]\n",
      " ...\n",
      " [0.0394 0.042 0.0446 ... 0.0068 0.0053 0.0087]\n",
      " [0.0187 0.0346 0.0168 ... 0.0115 0.0193 0.0157]\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]]\n",
      "________________________________________________________________________________________________________________________\n",
      "[[0.02 0.0371 0.0428 ... 0.0084 0.009 0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049 ... 0.0079 0.0036 0.0048]\n",
      " [0.026 0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "[[0.0123 0.0309 0.0169 ... 0.0092 0.0009 0.0044]\n",
      " [0.0177 0.03 0.0288 ... 0.0057 0.0032 0.0019]\n",
      " [0.0442 0.0477 0.0049 ... 0.0105 0.0059 0.0105]\n",
      " ...\n",
      " [0.013 0.012 0.0436 ... 0.0009 0.0033 0.0026]\n",
      " [0.0294 0.0123 0.0117 ... 0.0006 0.0081 0.0043]\n",
      " [0.0131 0.0387 0.0329 ... 0.0009 0.0015 0.0085]]\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for train, test in skf.split(x, y):\n",
    "    print(x[train])\n",
    "    print(x[test])\n",
    "    print('_' * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16718298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 0s 973us/step - loss: 0.3492 - accuracy: 0.8289\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0ABBBD4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.8006 - accuracy: 0.7619\n",
      "187/187 [==============================] - 0s 902us/step - loss: 0.3633 - accuracy: 0.8182\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0A75BA040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 989us/step - loss: 0.4240 - accuracy: 0.8571\n",
      "187/187 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.7807\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0ACDEE160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3136 - accuracy: 0.9048\n",
      "187/187 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8128\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0ACFBA820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8095\n",
      "187/187 [==============================] - 0s 976us/step - loss: 0.4080 - accuracy: 0.8289\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0AB6D4940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3588 - accuracy: 0.7619\n",
      "187/187 [==============================] - 0s 979us/step - loss: 0.4539 - accuracy: 0.8396\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0A8BD4790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.8095\n",
      "187/187 [==============================] - 0s 1ms/step - loss: 0.3566 - accuracy: 0.8289\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0A75BA040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4790 - accuracy: 0.7143\n",
      "187/187 [==============================] - 0s 1ms/step - loss: 0.3796 - accuracy: 0.8128\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0AA143550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4484 - accuracy: 0.7619\n",
      "188/188 [==============================] - 0s 958us/step - loss: 0.3932 - accuracy: 0.8245\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0ACEC50D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.7500\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.8457\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D0AA143820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7000\n",
      "\n",
      "10 fold accuracy : [0.761904776096344, 0.8571428656578064, 0.9047619104385376, 0.8095238208770752, 0.761904776096344, 0.8095238208770752, 0.7142857313156128, 0.761904776096344, 0.75, 0.699999988079071]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "\n",
    "for train, test in skf.split(x, y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim = 60, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics = ['accuracy']\n",
    "                 )\n",
    "    \n",
    "    model.fit(x[train].astype(float), y[train], epochs=30, batch_size=5, verbose = False)\n",
    "    model.fit(x[train].astype(float), y[train], epochs=1, batch_size=1)\n",
    "    \n",
    "    k_accuracy = model.evaluate(x[test].astype(float), y[test])[1]\n",
    "    accuracy.append(k_accuracy)\n",
    "\n",
    "print()\n",
    "print(f'{n_fold} fold accuracy : {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel-mypy",
   "language": "python",
   "name": "mypy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
